from __future__ import annotations

import json
import os
import re
import time
import random
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional, Union, Callable

try:
    from openai import OpenAI  # openai>=1.x
except Exception:
    OpenAI = None


# =========================
# å›ºå®šé…ç½®ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œï¼špython3 scripts/i18n_tool.pyï¼‰
# =========================
LANGS_FILE = Path("src/assets/languages.json")
DEFAULT_LOCALES_DIR_CANDIDATES = [Path("src/locales"), Path("locales")]

# ä½ çš„åŸºç¡€æ–‡ä»¶å®é™…æ˜¯ï¼šsrc/locales/zh-hans.json
BASE = "zh-hans"
FIRST_HOP = ["en", "zh-hant", "ja", "ko"]

MODEL = "gpt-4o-mini"

# âœ… å¹¶å‘çº¿ç¨‹æ•°ï¼ˆ1 = ä¸å¹¶å‘ï¼›>1 = å¤šçº¿ç¨‹å¹¶å‘ï¼‰
# å¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼šI18N_WORKERS=6 python3 scripts/i18n_tool.py
MAX_WORKERS = int(os.getenv("I18N_WORKERS", "6"))

CACHE_FILE = Path(".cache/i18n_translate_cache.json")
APIKEY_FILE = Path("scripts/apikey")

# âœ… ä¸ç¿»è¯‘ä¿æŠ¤è¯ï¼ˆæ•´å¥å‘½ä¸­ç›´æ¥ copyï¼›å¥å­å†…å‡ºç°ä¼šè¢«æ©ç ï¼Œç¿»å®Œè¿˜åŸï¼‰
# æ–‡ä»¶æ ¼å¼ï¼š["TreeHouse Tech", "ä¸Šæµ·æ ‘ä¸‹å°å±‹ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸", "sxxw.site"]
PROTECTED_TERMS_FILE = Path("scripts/protected_terms.json")


# =========================
# å ä½ç¬¦ä¿æŠ¤ & CJK å¤„ç†
# =========================
_CJK_RE = re.compile(r"[\u3400-\u9FFF\uF900-\uFAFF]")  # CJK è¡¨æ„ + å…¼å®¹åŒº
_PLACEHOLDER_RE = re.compile(
    r"(%\d*\$?[sd]|%\d*\.?\d*[df]|%@|{[^{}]+}|{[0-9]+}|{{[^{}]+}}|<\/?[^>]+>)"
)

def is_zh(code: str) -> bool:
    return (code or "").lower().startswith("zh")

def is_cjk(code: str) -> bool:
    c = (code or "").lower()
    return c.startswith("zh") or c.startswith("ja") or c.startswith("ko")

def ensure_no_cjk_when_forbidden(text: str, tgt_locale: str) -> str:
    if not text:
        return text
    if is_cjk(tgt_locale):
        return text
    return _CJK_RE.sub("", text).strip()

def extract_placeholders(s: str) -> List[str]:
    return _PLACEHOLDER_RE.findall(s or "")

def placeholders_equal(src: str, tgt: str) -> bool:
    return extract_placeholders(src) == extract_placeholders(tgt)

def mask_placeholders(text: str) -> Tuple[str, Dict[str, str]]:
    mapping: Dict[str, str] = {}
    idx = 0

    def repl(m: re.Match) -> str:
        nonlocal idx
        token = f"__PH{idx}__"
        mapping[token] = m.group(0)
        idx += 1
        return token

    return _PLACEHOLDER_RE.sub(repl, text), mapping

def unmask_placeholders(text: str, mapping: Dict[str, str]) -> str:
    for k, v in mapping.items():
        text = text.replace(k, v)
    return text


# =========================
# ä¿æŠ¤è¯ï¼šæ©ç /è¿˜åŸ
# =========================
_TERM_TOKEN_RE = re.compile(r"__TERM(\d+)__")

def load_protected_terms() -> List[str]:
    # env: I18N_PROTECTED_TERMS="foo,bar,baz"
    env = (os.getenv("I18N_PROTECTED_TERMS") or "").strip()
    terms: List[str] = []
    if env:
        terms.extend([x.strip() for x in env.split(",") if x.strip()])

    if PROTECTED_TERMS_FILE.exists():
        try:
            data = json.loads(PROTECTED_TERMS_FILE.read_text(encoding="utf-8"))
            if isinstance(data, list):
                terms.extend([str(x) for x in data if str(x).strip()])
        except Exception:
            pass

    # å»é‡ + é•¿åº¦é™åºï¼ˆé¿å…çŸ­è¯å…ˆæ›¿æ¢å¯¼è‡´é•¿è¯æ— æ³•åŒ¹é…ï¼‰
    uniq: List[str] = []
    seen = set()
    for t in terms:
        if t not in seen:
            uniq.append(t)
            seen.add(t)
    uniq.sort(key=len, reverse=True)
    return uniq

def mask_protected_terms(text: str, terms: List[str]) -> Tuple[str, Dict[str, str]]:
    """
    æŠŠ text é‡Œå‡ºç°çš„ä¿æŠ¤è¯æ›¿æ¢æˆ __TERM0__/__TERM1__...ï¼Œé¿å…è¢«æ¨¡å‹ç¿»è¯‘
    """
    if not text or not terms:
        return text, {}

    mapping: Dict[str, str] = {}
    out = text
    idx = 0
    for term in terms:
        if not term:
            continue
        if term in out:
            token = f"__TERM{idx}__"
            mapping[token] = term
            out = out.replace(term, token)
            idx += 1
    return out, mapping

def unmask_protected_terms(text: str, mapping: Dict[str, str]) -> str:
    if not text or not mapping:
        return text
    for k, v in mapping.items():
        text = text.replace(k, v)
    return text


# =========================
# Path tokenization + æ’åº Keyï¼ˆä»…ç”¨äºâ€œæ’åºé€‰é¡¹â€/å‰ç¼€åŒ¹é…ï¼‰
# =========================
Token = Union[str, int]

def parse_path_tokens(path: str) -> List[Token]:
    tokens: List[Token] = []
    i = 0
    while i < len(path):
        if path[i] == "[":
            j = path.index("]", i)
            tokens.append(int(path[i + 1 : j]))
            i = j + 1
        elif path[i] == ".":
            i += 1
        else:
            j = i
            while j < len(path) and path[j] not in ".[":
                j += 1
            tokens.append(path[i:j])
            i = j
    return tokens

def path_sort_key(path: str) -> Tuple:
    toks = parse_path_tokens(path)
    key = []
    for t in toks:
        if isinstance(t, int):
            key.append((0, t))
        else:
            key.append((1, t))
    return tuple(key)


# =========================
# JSON æ‰å¹³åŒ–ï¼ˆä¿æŒæ’å…¥é¡ºåºï¼‰
# æ³¨æ„ï¼šå¦‚æœæºæ–‡ä»¶æœ¬æ¥å°±æ˜¯å¹³é“º keyï¼ˆå« . æˆ– []ï¼‰ï¼Œè¿™é‡Œä¸ä¼šæ‹† key
# =========================
def flatten_json(obj: Any, prefix: str = "") -> List[Tuple[str, Any]]:
    out: List[Tuple[str, Any]] = []
    if isinstance(obj, dict):
        for k, v in obj.items():  # insertion-order
            p = f"{prefix}.{k}" if prefix else str(k)
            out.extend(flatten_json(v, p))
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            p = f"{prefix}[{i}]"
            out.extend(flatten_json(v, p))
    else:
        out.append((prefix, obj))
    return out

def pairs_to_flat_dict(pairs: List[Tuple[str, Any]]) -> Dict[str, Any]:
    d: Dict[str, Any] = {}
    for k, v in pairs:
        d[k] = v
    return d

def write_json_preserve_order(path: Path, obj: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")

def atomic_write_json(path: Path, obj: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.with_suffix(path.suffix + ".tmp")
    tmp.write_text(json.dumps(obj, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")
    tmp.replace(path)


# =========================
# locale æ–‡ä»¶å‘½åï¼ˆç»Ÿä¸€å°å†™ï¼Œé¿å…å¤§å°å†™å¹³å°å·®å¼‚ï¼‰
# =========================
def code_to_filename(code: str) -> str:
    return f"{(code or '').strip().lower()}.json"

def locale_path(locales_dir: Path, code: str) -> Path:
    return locales_dir / code_to_filename(code)


# =========================
# è¯­è¨€æ¸…å•è¯»å–
# =========================
@dataclass(frozen=True)
class LangSpec:
    code: str
    name: str
    fallbacks: List[str]
    rtl: bool

def load_languages() -> List[LangSpec]:
    data = json.loads(LANGS_FILE.read_text(encoding="utf-8"))
    out: List[LangSpec] = []
    for x in data:
        out.append(
            LangSpec(
                code=x["code"],
                name=x.get("name") or x["code"],
                fallbacks=x.get("fallbacks") or [],
                rtl=bool(x.get("rtl", False)),
            )
        )
    return out

def pick_locales_dir() -> Path:
    for p in DEFAULT_LOCALES_DIR_CANDIDATES:
        if p.exists():
            return p
    DEFAULT_LOCALES_DIR_CANDIDATES[0].mkdir(parents=True, exist_ok=True)
    return DEFAULT_LOCALES_DIR_CANDIDATES[0]


# =========================
# API Keyï¼ˆä¼˜å…ˆ envï¼Œå…¶æ¬¡è¯»å– scripts/apikeyï¼‰
# =========================
def read_api_key() -> str:
    env = (os.getenv("OPENAI_API_KEY") or "").strip()
    if env:
        return env
    if APIKEY_FILE.exists():
        key = APIKEY_FILE.read_text(encoding="utf-8").strip()
        if key:
            return key
    raise RuntimeError("æœªæ‰¾åˆ° OpenAI API Keyï¼šè¯·è®¾ç½® OPENAI_API_KEY æˆ–åœ¨ scripts/apikey å†™å…¥ key")


# =========================
# ç¼“å­˜
# =========================
def load_cache() -> Dict[str, str]:
    if not CACHE_FILE.exists():
        return {}
    try:
        return json.loads(CACHE_FILE.read_text(encoding="utf-8"))
    except Exception:
        return {}

def save_cache(cache: Dict[str, str]) -> None:
    CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
    CACHE_FILE.write_text(json.dumps(cache, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")

def cache_key(src_lang: str, tgt_lang: str, text: str) -> str:
    return f"{src_lang}|||{tgt_lang}|||{text}"


# =========================
# OpenCCï¼šä¸­æ–‡ç®€ç¹ç›´è½¬ï¼ˆå¯é€‰ï¼‰
# =========================
def opencc_convert_if_possible(src_text: str, src_lang: str, tgt_lang: str) -> Optional[str]:
    s = (src_lang or "").lower()
    t = (tgt_lang or "").lower()
    if not (s.startswith("zh") and t.startswith("zh")):
        return None
    try:
        import opencc  # pip install opencc
        if "hant" in t or t.endswith(("tw", "hk")):
            return opencc.OpenCC("s2t.json").convert(src_text)
        if "hans" in t or t.endswith(("cn",)):
            return opencc.OpenCC("t2s.json").convert(src_text)
    except Exception:
        return None
    return None


# =========================
# ç¿»è¯‘è¯·æ±‚ï¼ˆæ ¸å¿ƒï¼‰
# =========================
def build_system_prompt(src_lang_name: str, tgt_lang_name: str, tgt_code: str) -> str:
    rules = [
        "You are a senior localization translator.",
        f"Translate from {src_lang_name} to {tgt_lang_name}.",
        "Preserve brand names and URLs verbatim.",
        "Preserve placeholders/tokens EXACTLY (e.g., {name}, {0}, %d, %@, {{count}}, __PH0__, __TERM0__).",
        "Return ONLY valid JSON (no markdown, no extra text).",
        'JSON schema: {"items":[{"path":"...","text":"..."}]}',
        "Do not change any path value.",
    ]
    if not is_zh(tgt_code):
        rules.append("IMPORTANT: Do NOT use any Chinese characters in the output.")
    return " ".join(rules)

def call_openai_batch(
        client: OpenAI,
        model: str,
        src_lang_name: str,
        tgt_lang_name: str,
        tgt_code: str,
        batch: List[Tuple[str, str]],  # [(path, masked_text)]
        timeout: float = 60.0,
        max_retries: int = 4,
) -> Dict[str, str]:
    payload = {"items": [{"path": p, "text": t} for p, t in batch]}
    sys_prompt = build_system_prompt(src_lang_name, tgt_lang_name, tgt_code)

    last_err: Optional[Exception] = None
    for attempt in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
                ],
                temperature=0.2 if attempt == 0 else 0.0,
                top_p=0.9,
                timeout=timeout,
            )
            out = (resp.choices[0].message.content or "").strip()
            data = json.loads(out)

            items = data.get("items") or []
            result: Dict[str, str] = {}
            for it in items:
                p = it.get("path")
                txt = it.get("text")
                if isinstance(p, str) and isinstance(txt, str):
                    result[p] = txt.strip()

            if len(result) >= max(1, int(0.85 * len(batch))):
                return result

            raise ValueError("Batch parse ok but too few items returned.")
        except Exception as e:
            last_err = e
            time.sleep((2 ** attempt) + random.uniform(0, 0.25))

    raise RuntimeError(f"Batch translate failed: {last_err}")


# =========================
# æ—¥å¿—å›è°ƒï¼šæ‰“å° key / åŸæ–‡ / ç›®æ ‡è¯­è¨€ / è¯‘æ–‡
# =========================
def default_log_translation(tgt_code: str, path: str, src_text: str, tgt_text: str) -> None:
    print(f"[{tgt_code}] {path}", flush=True)
    print(f"  SRC: {src_text}", flush=True)
    print(f"  TGT: {tgt_text}\n", flush=True)


def translate_tree(
        *,
        base_obj: Any,
        base_code: str,
        src_lang_name: str,
        tgt_code: str,
        tgt_lang_name: str,
        model: str,
        api_key: str,
        existing_obj: Optional[Any],
        cache: Dict[str, str],
        force_full: bool,
        protected_terms: Optional[List[str]] = None,
        out_path: Optional[Path] = None,   # âœ… è¾¹ç¿»è¯‘è¾¹å†™
        log_translation: Optional[Callable[[str, str, str, str], None]] = None,
) -> Any:
    """
    force_full=Falseï¼šå¢é‡è¿½åŠ ï¼ˆåªè¡¥ç¼ºå¤±/ç©ºå­—æ®µï¼‰ï¼Œæ–°å¢å­—æ®µè¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾
    force_full=True ï¼šå…¨é‡è¦†ç›–ï¼ˆæŒ‰ base çš„éå†é¡ºåºç”Ÿæˆï¼‰

    è¾“å‡ºï¼šæ°¸è¿œæ˜¯â€œå¹³é“º JSONâ€ï¼ˆä¸åˆ†çº§ï¼‰ï¼Œkey ä¸æºæ–‡ä»¶ä¸€è‡´
    å¹¶å‘ï¼šMAX_WORKERS
    è¿›åº¦ï¼šcompleted/total å•è°ƒé€’å¢ï¼ˆå¹¶å‘ä¸ä¼šâ€œçœ‹èµ·æ¥é”™ä¹±â€ï¼‰
    è¾¹ç¿»è¯‘è¾¹å†™ï¼šæ¯å®Œæˆä¸€æ¡ atomic å†™å› out_path
    ä¿æŠ¤è¯ï¼šæ•´å¥å‘½ä¸­ç›´æ¥ copyï¼›å¥å­ä¸­å‡ºç°ä¼šæ©ç ï¼Œç¿»å®Œè¿˜åŸ
    """
    log_translation = log_translation or default_log_translation
    protected_terms = protected_terms or []

    # ---- ä¸­æ–‡åŒè¯­ç³»ï¼šOpenCC ç›´è½¬ï¼ˆå¦‚æœå¯ç”¨ï¼Œé GPTï¼‰----
    if is_zh(base_code) and is_zh(tgt_code):
        base_pairs = flatten_json(base_obj)
        out_pairs: List[Tuple[str, Any]] = []
        for path, val in base_pairs:
            if isinstance(val, str):
                direct = opencc_convert_if_possible(val, base_code, tgt_code)
                out_pairs.append((path, direct if direct is not None else val))
            else:
                out_pairs.append((path, val))
        out_dict = pairs_to_flat_dict(out_pairs)
        if out_path:
            atomic_write_json(out_path, out_dict)
        return out_dict

    if not OpenAI:
        raise SystemExit("OpenAI SDK æœªå®‰è£…ï¼špip install openai>=1.0.0")

    base_pairs = flatten_json(base_obj)
    base_map: Dict[str, Any] = {p: v for p, v in base_pairs}

    existing_pairs = flatten_json(existing_obj) if existing_obj is not None else []
    existing_map: Dict[str, Any] = {p: v for p, v in existing_pairs}

    # =========================
    # âœ… å…ˆæ­å‡ºæœ€ç»ˆ out_dictï¼ˆç¡®ä¿å†™å‡ºçš„ key é¡ºåºç¨³å®šï¼‰
    # =========================
    out_dict: Dict[str, Any] = {}

    if force_full:
        for path, val in base_pairs:
            if isinstance(val, str):
                out_dict[path] = ""
            else:
                out_dict[path] = val
    else:
        # å…ˆä¿ç•™ existing çš„é¡ºåº
        for p, v in existing_pairs:
            out_dict[p] = v

        # å†è¡¥ base ä¸­ç¼ºå¤±çš„éå­—ç¬¦ä¸²é¡¹
        for path, val in base_pairs:
            if path in out_dict:
                continue
            if not isinstance(val, str):
                out_dict[path] = val

        # å†è¡¥ base ä¸­ç¼ºå¤±çš„å­—ç¬¦ä¸²é¡¹å ä½ï¼ˆä¿è¯åç»­è¦†ç›–ä¸æ”¹å˜é¡ºåºï¼‰
        for path, val in base_pairs:
            if path in out_dict:
                continue
            if isinstance(val, str):
                out_dict[path] = ""

    # =========================
    # todo + æ˜ å°„ï¼ˆå ä½ç¬¦/ä¿æŠ¤è¯ï¼‰
    # =========================
    todo: List[Tuple[int, str, str]] = []  # (seq, path, masked_src)
    masked_maps: Dict[str, Dict[str, str]] = {}
    term_maps: Dict[str, Dict[str, str]] = {}

    seq = 0
    for path, val in base_pairs:
        if not isinstance(val, str):
            continue

        # å¢é‡ï¼šå·²æœ‰éç©ºè¯‘æ–‡å°±è·³è¿‡
        if not force_full:
            cur = existing_map.get(path, None)
            if isinstance(cur, str) and cur.strip() != "":
                continue

        # âœ… ä¾‹å¤–ï¼šæ•´å¥å‘½ä¸­ä¿æŠ¤è¯ -> ç›´æ¥ copyï¼Œä¸èµ°ç¿»è¯‘
        if val.strip() in protected_terms:
            out_dict[path] = val
            ck = cache_key(src_lang_name, tgt_code, val)
            cache[ck] = val
            continue

        # cache å‘½ä¸­ -> ç›´æ¥å¡«
        ck = cache_key(src_lang_name, tgt_code, val)
        if ck in cache:
            out_dict[path] = cache[ck]
            continue

        # âœ… å…ˆå ä½ç¬¦æ©ç ï¼Œå†ä¿æŠ¤è¯æ©ç 
        masked, ph_map = mask_placeholders(val)
        masked, tm_map = mask_protected_terms(masked, protected_terms)

        masked_maps[path] = ph_map
        term_maps[path] = tm_map

        seq += 1
        todo.append((seq, path, masked))

    # å…ˆå†™ä¸€æ¬¡éª¨æ¶ï¼ˆå« cache/ä¿æŠ¤è¯ç›´æ‹·è´ç»“æœï¼‰ï¼Œæ–¹ä¾¿ä½ â€œè¾¹ç¿»è¯‘è¾¹çœ‹åˆ°æ–‡ä»¶å˜åŒ–â€
    if out_path:
        atomic_write_json(out_path, out_dict)

    total = len(todo)
    mode = "å¹¶å‘" if MAX_WORKERS > 1 else "å•çº¿ç¨‹"
    if total > 0:
        print(f"ğŸ§© [{tgt_code}] å¾…ç¿»è¯‘ {total} æ¡ï¼ˆ{mode}ï¼Œæ¯æ¡å®Œæˆå³æ—¥å¿— + å†™æ–‡ä»¶ï¼‰", flush=True)

    lock = threading.Lock()
    completed = 0
    succeeded = 0

    def postprocess(path: str, masked_tgt: str) -> Optional[str]:
        src_text = base_map.get(path)
        if not isinstance(src_text, str):
            return None

        # å…ˆè¿˜åŸå ä½ç¬¦ï¼Œå†è¿˜åŸä¿æŠ¤è¯
        unmasked = unmask_placeholders(masked_tgt, masked_maps.get(path, {}))
        unmasked = unmask_protected_terms(unmasked, term_maps.get(path, {}))

        # é CJK ç›®æ ‡è¯­è¨€å»æ‰ä¸­æ–‡ï¼ˆä¿ç•™ä½ åŸé€»è¾‘ï¼‰
        unmasked = ensure_no_cjk_when_forbidden(unmasked, tgt_code)

        # å ä½ç¬¦ä¸€è‡´æ€§ä¿®å¤
        if extract_placeholders(src_text) and not placeholders_equal(src_text, unmasked):
            src_ph = extract_placeholders(src_text)
            it = iter(src_ph)

            def repl(m: re.Match) -> str:
                return next(it, m.group(0))

            unmasked = _PLACEHOLDER_RE.sub(repl, unmasked)

        return unmasked

    # æ¯çº¿ç¨‹å¤ç”¨ä¸€ä¸ª client
    _tl = threading.local()

    def get_client() -> OpenAI:
        c = getattr(_tl, "client", None)
        if c is None:
            _tl.client = OpenAI(api_key=api_key)
            c = _tl.client
        return c

    def translate_one(path: str, masked_src: str) -> Optional[str]:
        client = get_client()
        out_map = call_openai_batch(
            client=client,
            model=model,
            src_lang_name=src_lang_name,
            tgt_lang_name=tgt_lang_name,
            tgt_code=tgt_code,
            batch=[(path, masked_src)],  # âœ… æ°¸è¿œå•æ¡
        )
        if path not in out_map:
            return None
        return postprocess(path, out_map[path])

    def apply_success(seq_no: int, path: str, final: str) -> None:
        nonlocal succeeded
        src_text = base_map.get(path)

        with lock:
            out_dict[path] = final
            if isinstance(src_text, str):
                cache[cache_key(src_lang_name, tgt_code, src_text)] = final
            succeeded += 1
            if out_path:
                atomic_write_json(out_path, out_dict)

        log_translation(tgt_code, path, src_text if isinstance(src_text, str) else "", final)

    def tick_complete(ok: bool, seq_no: int, path: str) -> None:
        nonlocal completed
        with lock:
            completed += 1
            c = completed
        status = "âœ…" if ok else "âš ï¸"
        print(f"{status} [{tgt_code}] ({c}/{total})  {path}  (seq:{seq_no})", flush=True)

    # =========================
    # æ‰§è¡Œï¼šå•çº¿ç¨‹ or å¹¶å‘
    # =========================
    if MAX_WORKERS <= 1:
        for seq_no, path, masked_src in todo:
            print(f"â³ [{tgt_code}] start (seq:{seq_no}/{total})  {path}", flush=True)
            final = translate_one(path, masked_src)
            if final is None:
                tick_complete(False, seq_no, path)
                continue
            apply_success(seq_no, path, final)
            tick_complete(True, seq_no, path)
    else:
        from concurrent.futures import ThreadPoolExecutor, as_completed

        def worker(seq_no: int, path: str, masked_src: str) -> Tuple[int, str, Optional[str]]:
            final = translate_one(path, masked_src)
            return seq_no, path, final

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            futures = [ex.submit(worker, seq_no, path, masked_src) for seq_no, path, masked_src in todo]
            for fut in as_completed(futures):
                seq_no, path, final = fut.result()
                if final is None:
                    tick_complete(False, seq_no, path)
                    continue
                apply_success(seq_no, path, final)
                tick_complete(True, seq_no, path)

    # æœ€ç»ˆä¿é™©è½ç›˜ + ç¼“å­˜ä¿å­˜äº¤ç»™ä¸Šå±‚
    if out_path:
        with lock:
            atomic_write_json(out_path, out_dict)

    if total > 0:
        print(f"ğŸ [{tgt_code}] å®Œæˆï¼šæˆåŠŸ {succeeded}/{total}", flush=True)

    return out_dict


# =========================
# ç¬¬ä¸€é˜¶æ®µ / ç¬¬äºŒé˜¶æ®µï¼ˆæ”¯æŒå¢é‡è¿½åŠ  & å…¨é‡è¦†ç›–ï¼‰
# =========================
def build_first_hop(locales_dir: Path, api_key: str, force_full: bool, protected_terms: List[str]) -> None:
    langs = load_languages()
    lang_by_code = {l.code.lower(): l for l in langs}
    cache = load_cache()

    base_path = locale_path(locales_dir, BASE)
    if not base_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°åŸºç¡€è¯­è¨€æ–‡ä»¶ï¼š{base_path}")

    base_obj = json.loads(base_path.read_text(encoding="utf-8"))

    def lang_name(code: str) -> str:
        return (lang_by_code.get(code.lower()).name if code.lower() in lang_by_code else code)

    for code in FIRST_HOP:
        out_path = locale_path(locales_dir, code)
        existing = None
        if out_path.exists() and not force_full:
            existing = json.loads(out_path.read_text(encoding="utf-8"))

        translate_tree(
            base_obj=base_obj,
            base_code=BASE,
            src_lang_name="Chinese (Simplified)",
            tgt_code=code,
            tgt_lang_name=f"{lang_name(code)} [{code}]",
            model=MODEL,
            api_key=api_key,
            existing_obj=existing,
            cache=cache,
            force_full=force_full,
            protected_terms=protected_terms,
            out_path=out_path,  # âœ… è¾¹ç¿»è¯‘è¾¹å†™
        )

        mode = "å…¨é‡è¦†ç›–" if force_full else "å¢é‡è¿½åŠ "
        print(f"âœ… ç¬¬ä¸€é˜¶æ®µ{mode}ï¼š{out_path}", flush=True)

    save_cache(cache)
    print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ï¼š{CACHE_FILE}", flush=True)


def build_second_hop_from_en(locales_dir: Path, api_key: str, force_full: bool, protected_terms: List[str]) -> None:
    langs = load_languages()
    cache = load_cache()

    en_path = locale_path(locales_dir, "en")
    if not en_path.exists():
        raise RuntimeError("ç¬¬äºŒé˜¶æ®µéœ€è¦ en.jsonï¼Œä½†æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œç¬¬ä¸€é˜¶æ®µç”Ÿæˆ en.jsonã€‚")

    en_obj = json.loads(en_path.read_text(encoding="utf-8"))

    excluded_lower = set([BASE.lower()] + [x.lower() for x in FIRST_HOP])

    for l in langs:
        code = l.code
        if code.lower() in excluded_lower:
            continue

        out_path = locale_path(locales_dir, code)
        existing = None
        if out_path.exists() and not force_full:
            existing = json.loads(out_path.read_text(encoding="utf-8"))

        # è‹±è¯­åœ°åŒºç ï¼šç›´æ¥å¤ç”¨ enï¼ˆä¸èµ°ç¿»è¯‘ï¼‰
        if code.lower().startswith("en-"):
            write_json_preserve_order(out_path, en_obj)
            print(f"ğŸŸ¦ å¤ç”¨ enï¼š{out_path}", flush=True)
            continue

        # fallbacksï¼šä¼˜å…ˆå¤ç”¨ï¼ˆçœé’±ï¼‰
        reused = False
        for fb in (l.fallbacks or []):
            fb_path = locale_path(locales_dir, fb)
            if fb_path.exists():
                fb_obj = json.loads(fb_path.read_text(encoding="utf-8"))
                write_json_preserve_order(out_path, fb_obj)
                print(f"ğŸŸ¨ å¤ç”¨ fallback {fb}ï¼š{out_path}", flush=True)
                reused = True
                break
        if reused:
            continue

        translate_tree(
            base_obj=en_obj,
            base_code="en",
            src_lang_name="English",
            tgt_code=code,
            tgt_lang_name=f"{l.name} [{code}]",
            model=MODEL,
            api_key=api_key,
            existing_obj=existing,
            cache=cache,
            force_full=force_full,
            protected_terms=protected_terms,
            out_path=out_path,  # âœ… è¾¹ç¿»è¯‘è¾¹å†™
        )

        mode = "å…¨é‡è¦†ç›–" if force_full else "å¢é‡è¿½åŠ "
        print(f"âœ… ç¬¬äºŒé˜¶æ®µ{mode}ï¼š{out_path}", flush=True)

    save_cache(cache)
    print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ï¼š{CACHE_FILE}", flush=True)


# =========================
# æ¸…ç†ï¼šæ ¹æ® key åˆ é™¤ç¿»è¯‘å­—æ®µï¼ˆä¸åŠ¨ BASEï¼‰
# - ä¸æ’åºï¼Œä»…åˆ é™¤ï¼›ä¿ç•™åŸé¡ºåº
# =========================
def normalize_key_patterns(raw: str) -> List[Tuple[str, bool]]:
    parts = [x.strip() for x in raw.split(",") if x.strip()]
    out: List[Tuple[str, bool]] = []
    for p in parts:
        if p.endswith(".*"):
            out.append((p[:-2], True))
        elif p.endswith("."):
            out.append((p[:-1], True))
        elif p.endswith("*"):
            out.append((p[:-1], True))
        else:
            out.append((p, False))
    return out

def should_remove(path: str, patterns: List[Tuple[str, bool]]) -> bool:
    for pat, is_prefix in patterns:
        if is_prefix:
            if path == pat or path.startswith(pat + ".") or path.startswith(pat + "["):
                return True
        else:
            if path == pat:
                return True
    return False

def clean_translations_by_key(locales_dir: Path, key_patterns_raw: str) -> None:
    patterns = normalize_key_patterns(key_patterns_raw)
    if not patterns:
        print("æœªæä¾›æœ‰æ•ˆ keyã€‚", flush=True)
        return

    base_name = code_to_filename(BASE)
    files = [p for p in locales_dir.glob("*.json") if p.name != base_name]

    if not files:
        print("æ²¡æœ‰å¯æ¸…ç†çš„ç¿»è¯‘æ–‡ä»¶ï¼ˆé™¤ base å¤–ï¼‰ã€‚", flush=True)
        return

    total_removed = 0
    for fp in files:
        obj = json.loads(fp.read_text(encoding="utf-8"))
        flat = flatten_json(obj)

        kept: List[Tuple[str, Any]] = []
        removed = 0
        for path, val in flat:
            if should_remove(path, patterns):
                removed += 1
                continue
            kept.append((path, val))

        if removed > 0:
            total_removed += removed
            new_obj = pairs_to_flat_dict(kept)  # ä»ç„¶å†™å¹³é“º JSON
            write_json_preserve_order(fp, new_obj)
            print(f"ğŸ§½ {fp.name}: åˆ é™¤ {removed} é¡¹", flush=True)
        else:
            print(f"ğŸ§¼ {fp.name}: æ— åŒ¹é…é¡¹ï¼Œè·³è¿‡", flush=True)

    print(f"âœ… æ¸…ç†å®Œæˆï¼šå…±åˆ é™¤ {total_removed} é¡¹ï¼ˆä¸å½±å“ {base_name}ï¼‰", flush=True)


# =========================
# æ’åºï¼šåªæœ‰è¿è¡Œæ­¤é€‰é¡¹æ‰æ’åº
# - è¾“å‡ºä»ç„¶æ˜¯å¹³é“º JSONï¼ˆä¸åˆ†çº§ï¼‰
# =========================
def sort_locale_file(path: Path) -> int:
    obj = json.loads(path.read_text(encoding="utf-8"))
    pairs = flatten_json(obj)
    before = [p for p, _ in pairs]
    pairs.sort(key=lambda x: path_sort_key(x[0]))
    after = [p for p, _ in pairs]
    changed = 1 if before != after else 0
    sorted_obj = pairs_to_flat_dict(pairs)
    write_json_preserve_order(path, sorted_obj)
    return changed

def sort_locales(locales_dir: Path, include_base: bool = False) -> None:
    base_name = code_to_filename(BASE)
    files = [p for p in locales_dir.glob("*.json") if include_base or p.name != base_name]
    if not files:
        print("æ²¡æœ‰å¯æ’åºçš„ json æ–‡ä»¶ã€‚", flush=True)
        return

    changed_count = 0
    for fp in files:
        changed_count += sort_locale_file(fp)
        print(f"ğŸ”§ å·²æ’åºï¼š{fp.name}", flush=True)
    print(f"âœ… æ’åºå®Œæˆï¼šå¤„ç† {len(files)} ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­é¡ºåºå‘ç”Ÿå˜åŒ–çš„ {changed_count} ä¸ª", flush=True)


# =========================
# èœå•
# =========================
def menu() -> None:
    if not LANGS_FILE.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {LANGS_FILE}ï¼ˆè¯·ç¡®è®¤è·¯å¾„æ­£ç¡®ï¼‰")

    locales_dir = pick_locales_dir()

    while True:
        protected_terms = load_protected_terms()

        print("\n========== i18n å·¥å…· ==========", flush=True)
        print(f"LANGS_FILE : {LANGS_FILE}", flush=True)
        print(f"locales_dir: {locales_dir}", flush=True)
        print(f"BASE       : {BASE} ({code_to_filename(BASE)})", flush=True)
        print(f"MODEL      : {MODEL}", flush=True)
        print(f"WORKERS    : {MAX_WORKERS}  (env: I18N_WORKERS)", flush=True)
        print(f"PROTECTED  : {len(protected_terms)}  ({PROTECTED_TERMS_FILE} / env: I18N_PROTECTED_TERMS)", flush=True)
        print("--------------------------------", flush=True)
        print("1) ç¬¬ä¸€é˜¶æ®µï¼ˆå¢é‡è¿½åŠ ï¼‰ï¼šzh-hans â†’ en / zh-hant / ja / ko", flush=True)
        print("2) ç¬¬ä¸€é˜¶æ®µï¼ˆå…¨é‡è¦†ç›–ï¼‰ï¼šzh-hans â†’ en / zh-hant / ja / ko", flush=True)
        print("3) ç¬¬äºŒé˜¶æ®µï¼ˆå¢é‡è¿½åŠ ï¼‰ï¼šen â†’ å…¶å®ƒè¯­è¨€ï¼ˆæ’é™¤ en/zh-hant/ja/koï¼‰", flush=True)
        print("4) ç¬¬äºŒé˜¶æ®µï¼ˆå…¨é‡è¦†ç›–ï¼‰ï¼šen â†’ å…¶å®ƒè¯­è¨€ï¼ˆæ’é™¤ en/zh-hant/ja/koï¼‰", flush=True)
        print("5) æ ¹æ® key æ¸…ç†ç¿»è¯‘å­—æ®µï¼ˆä¸åŠ¨ baseï¼Œä¸æ’åºï¼‰", flush=True)
        print("6) æ’åºç¿»è¯‘æ–‡ä»¶ï¼ˆä»…æ­¤é€‰é¡¹æ‰æ’åºï¼‰", flush=True)
        print("7) é€€å‡º", flush=True)

        choice = input("é€‰æ‹©æ“ä½œ (1/2/3/4/5/6/7): ").strip()

        if choice in {"1", "2", "3", "4"}:
            api_key = read_api_key()
            if not api_key:
                print("ç¼ºå°‘ API Keyï¼Œå·²å–æ¶ˆã€‚", flush=True)
                continue

        if choice == "1":
            build_first_hop(locales_dir, api_key=api_key, force_full=False, protected_terms=protected_terms)

        elif choice == "2":
            build_first_hop(locales_dir, api_key=api_key, force_full=True, protected_terms=protected_terms)

        elif choice == "3":
            build_second_hop_from_en(locales_dir, api_key=api_key, force_full=False, protected_terms=protected_terms)

        elif choice == "4":
            build_second_hop_from_en(locales_dir, api_key=api_key, force_full=True, protected_terms=protected_terms)

        elif choice == "5":
            raw = input("è¾“å…¥è¦æ¸…ç†çš„ keyï¼ˆé€—å·åˆ†éš”ï¼›æ”¯æŒå‰ç¼€ home.* æˆ– home. æˆ– home*ï¼‰ï¼š\n> ").strip()
            clean_translations_by_key(locales_dir, raw)

        elif choice == "6":
            inc_base = input("æ˜¯å¦ä¹Ÿæ’åº baseï¼ˆzh-hans.jsonï¼‰ï¼Ÿ(y/N): ").strip().lower() == "y"
            sort_locales(locales_dir, include_base=inc_base)

        elif choice == "7":
            print("Bye.", flush=True)
            return

        else:
            print("æ— æ•ˆé€‰æ‹©ã€‚", flush=True)


if __name__ == "__main__":
    menu()
